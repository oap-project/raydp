{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Taxi Fare Prediction with RayDP and Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import os\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "import raydp\n",
    "from raydp.torch.estimator import TorchEstimator\n",
    "from raydp.utils import random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize or connect to existed Ray cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly, You need to init or connect to a ray cluster. Note that you should set include_java to True.\n",
    "# For more config info in ray, please refer the ray doc. https://docs.ray.io/en/latest/package-ref.html\n",
    "# ray.init(address=\"auto\", redis_password=\"123\")\n",
    "ray.init(include_java=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After initialize ray cluster, you can use the raydp api to get a spark session\n",
    "app_name = \"NYC Taxi Fare Prediction with RayDP\"\n",
    "num_executors = 4\n",
    "cores_per_executor = 1\n",
    "memory_per_executor = \"2GB\"\n",
    "spark = raydp.init_spark(app_name, num_executors, cores_per_executor, memory_per_executor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed data preprocessing with pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then you can code as you are using spark\n",
    "# The dataset can be downloaded from https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/data\n",
    "# Here we just use a subset of the training data\n",
    "train = spark.read.format(\"csv\").option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(\"/mnt/DP_disk8/nyc_train.csv\")\n",
    "\n",
    "# Set spark timezone for processing datetime\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the outlier\n",
    "def clean_up(data):\n",
    "    \n",
    "    data = data.filter(col('pickup_longitude')<=-72) \\\n",
    "            .filter(col('pickup_longitude')>=-76) \\\n",
    "            .filter(col('dropoff_longitude')<=-72) \\\n",
    "            .filter(col('dropoff_longitude')>=-76) \\\n",
    "            .filter(col('pickup_latitude')<=42) \\\n",
    "            .filter(col('pickup_latitude')>=38) \\\n",
    "            .filter(col('dropoff_latitude')<=42) \\\n",
    "            .filter(col('dropoff_latitude')>=38) \\\n",
    "            .filter(col('passenger_count')<=6) \\\n",
    "            .filter(col('passenger_count')>=1) \\\n",
    "            .filter(col('fare_amount') > 0) \\\n",
    "            .filter(col('fare_amount') < 250) \\\n",
    "            .filter(col('dropoff_longitude') != col('pickup_longitude')) \\\n",
    "            .filter(col('dropoff_latitude') != col('pickup_latitude')) \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add time related features\n",
    "def add_time_features(data):\n",
    "    \n",
    "    data = data.withColumn(\"day\", dayofmonth(col(\"pickup_datetime\")))\n",
    "    data = data.withColumn(\"hour_of_day\", hour(col(\"pickup_datetime\")))\n",
    "    data = data.withColumn(\"day_of_week\", dayofweek(col(\"pickup_datetime\"))-2)\n",
    "    data = data.withColumn(\"week_of_year\", weekofyear(col(\"pickup_datetime\")))\n",
    "    data = data.withColumn(\"month_of_year\", month(col(\"pickup_datetime\")))\n",
    "    data = data.withColumn(\"quarter_of_year\", quarter(col(\"pickup_datetime\")))\n",
    "    data = data.withColumn(\"year\", year(col(\"pickup_datetime\")))\n",
    "    \n",
    "    @udf(\"int\")\n",
    "    def night(hour, weekday):\n",
    "        if ((hour <= 20) and (hour >= 16) and (weekday < 5)):\n",
    "            return int(1)\n",
    "        else:\n",
    "            return int(0)\n",
    "\n",
    "    @udf(\"int\")\n",
    "    def late_night(hour):\n",
    "        if ((hour <= 6) and (hour >= 20)):\n",
    "            return int(1)\n",
    "        else:\n",
    "            return int(0)\n",
    "    data = data.withColumn(\"night\", night(\"hour_of_day\", \"day_of_week\"))\n",
    "    data = data.withColumn(\"late_night\", late_night(\"hour_of_day\"))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add distance related features\n",
    "def add_distance_features(data):\n",
    "\n",
    "    @udf(\"float\")\n",
    "    def manhattan(lat1, lon1, lat2, lon2):\n",
    "        return float(np.abs(lat2 - lat1) + np.abs(lon2 - lon1))\n",
    "    \n",
    "    # Location of NYC downtown\n",
    "    ny = (-74.0063889, 40.7141667)\n",
    "    # Location of the three airport in NYC\n",
    "    jfk = (-73.7822222222, 40.6441666667)\n",
    "    ewr = (-74.175, 40.69)\n",
    "    lgr = (-73.87, 40.77)\n",
    "    \n",
    "    # Features about the distance between pickup/dropoff and airport\n",
    "    data = data.withColumn(\"abs_diff_longitude\", abs(col(\"dropoff_longitude\")-col(\"pickup_longitude\"))) \\\n",
    "            .withColumn(\"abs_diff_latitude\", abs(col(\"dropoff_latitude\") - col(\"pickup_latitude\")))\n",
    "    data = data.withColumn(\"manhattan\", col(\"abs_diff_latitude\")+col(\"abs_diff_longitude\"))\n",
    "    data = data.withColumn(\"pickup_distance_jfk\", manhattan(\"pickup_longitude\", \"pickup_latitude\", lit(jfk[0]), lit(jfk[1])))\n",
    "    data = data.withColumn(\"dropoff_distance_jfk\", manhattan(\"dropoff_longitude\", \"dropoff_latitude\", lit(jfk[0]), lit(jfk[1])))\n",
    "    data = data.withColumn(\"pickup_distance_ewr\", manhattan(\"pickup_longitude\", \"pickup_latitude\", lit(ewr[0]), lit(ewr[1])))\n",
    "    data = data.withColumn(\"dropoff_distance_ewr\", manhattan(\"dropoff_longitude\", \"dropoff_latitude\", lit(ewr[0]), lit(ewr[1])))\n",
    "    data = data.withColumn(\"pickup_distance_lgr\", manhattan(\"pickup_longitude\", \"pickup_latitude\", lit(lgr[0]), lit(lgr[1])))\n",
    "    data = data.withColumn(\"dropoff_distance_lgr\", manhattan(\"dropoff_longitude\", \"dropoff_latitude\", lit(lgr[0]), lit(lgr[1])))\n",
    "    data = data.withColumn(\"pickup_distance_downtown\", manhattan(\"pickup_longitude\", \"pickup_latitude\", lit(ny[0]), lit(ny[1])))\n",
    "    data = data.withColumn(\"dropoff_distance_downtown\", manhattan(\"dropoff_longitude\", \"dropoff_latitude\", lit(ny[0]), lit(ny[1])))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unused features\n",
    "def drop_col(data):\n",
    "    \n",
    "    data = data.drop(\"pickup_datetime\") \\\n",
    "            .drop(\"pickup_longitude\") \\\n",
    "            .drop(\"pickup_latitude\") \\\n",
    "            .drop(\"dropoff_longitude\") \\\n",
    "            .drop(\"dropoff_latitude\") \\\n",
    "            .drop(\"passenger_count\") \\\n",
    "            .drop(\"key\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = clean_up(train)\n",
    "\n",
    "train_data = add_time_features(train_data)\n",
    "\n",
    "train_data = add_distance_features(train_data)\n",
    "\n",
    "train_data = drop_col(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train_dataset and test_dataset\n",
    "train_df, test_df = random_split(train_data, [0.9, 0.1])\n",
    "features = [field.name for field in list(train_df.schema) if field.name != \"fare_amount\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model, loss function and optimizer\n",
    "class NYC_Model(nn.Module):\n",
    "    def __init__(self, cols):\n",
    "        super(NYC_Model, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(cols, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 16)\n",
    "        self.fc5 = nn.Linear(16, 1)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.bn4 = nn.BatchNorm1d(16)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.bn4(x)\n",
    "        x = self.fc5(x)\n",
    "        \n",
    "        return x.squeeze(1)\n",
    "\n",
    "nyc_model = NYC_Model(len(features))\n",
    "criterion = nn.SmoothL1Loss()\n",
    "optimizer = torch.optim.Adam(nyc_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a distributed estimator based on the raydp api\n",
    "estimator = TorchEstimator(num_workers=4, model=nyc_model, optimizer=optimizer, loss=criterion,\n",
    "                            feature_columns=features, label_column=\"fare_amount\", batch_size=256, num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "estimator.fit_on_spark(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "estimator.evaluate_on_spark(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutdown raydp and ray\n",
    "estimator.shutdown()\n",
    "raydp.stop_spark()\n",
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ray] *",
   "language": "python",
   "name": "conda-env-ray-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
