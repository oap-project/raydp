{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Taxi Fare Prediction with RayDP and Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import os\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "import raydp\n",
    "from raydp.tf import TFEstimator\n",
    "from raydp.utils import random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize or connect to existed Ray cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, you need to init or connect to a ray cluster. Note that you should set include_java to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For more config info in ray, please refer the ray doc. https://docs.ray.io/en/latest/package-ref.html\n",
    "# ray.init(address=\"auto\", redis_password=\"123\")\n",
    "ray.init(include_java=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After initializing ray cluster, you can use the raydp api to get a spark session  \n",
    "`init_spark` take 4 required parameters and 1 optional parameters:  \n",
    "1. app_name: the application name\n",
    "2. num_executors: number of executors for spark application\n",
    "3. cores_per_executor: number of cores for each executor\n",
    "4. executor_memory: memory size for each executor \n",
    "5. config[option]: extra config for spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_name = \"NYC_Taxi Fare Prediction with RayDP\"\n",
    "num_executors = 4\n",
    "cores_per_executor = 1\n",
    "memory_per_executor = \"2GB\"\n",
    "spark = raydp.init_spark(app_name, num_executors, cores_per_executor, memory_per_executor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed data preprocessing with pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can do distributed data processing with spark session  \n",
    "You can download the dataset from https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we just use a subset of the training data\n",
    "train = spark.read.format(\"csv\").option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(\"/mnt/DP_disk8/nyc_train_1.csv\")\n",
    "\n",
    "# Set spark timezone for processing datetime\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the outlier\n",
    "def clean_up(data):\n",
    "    \n",
    "    data = data.filter(col('pickup_longitude')<=-72) \\\n",
    "            .filter(col('pickup_longitude')>=-76) \\\n",
    "            .filter(col('dropoff_longitude')<=-72) \\\n",
    "            .filter(col('dropoff_longitude')>=-76) \\\n",
    "            .filter(col('pickup_latitude')<=42) \\\n",
    "            .filter(col('pickup_latitude')>=38) \\\n",
    "            .filter(col('dropoff_latitude')<=42) \\\n",
    "            .filter(col('dropoff_latitude')>=38) \\\n",
    "            .filter(col('passenger_count')<=6) \\\n",
    "            .filter(col('passenger_count')>=1) \\\n",
    "            .filter(col('fare_amount') > 0) \\\n",
    "            .filter(col('fare_amount') < 250) \\\n",
    "            .filter(col('dropoff_longitude') != col('pickup_longitude')) \\\n",
    "            .filter(col('dropoff_latitude') != col('pickup_latitude')) \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add time related features\n",
    "def add_time_features(data):\n",
    "    \n",
    "    data = data.withColumn(\"day\", dayofmonth(col(\"pickup_datetime\")))\n",
    "    data = data.withColumn(\"hour_of_day\", hour(col(\"pickup_datetime\")))\n",
    "    data = data.withColumn(\"day_of_week\", dayofweek(col(\"pickup_datetime\"))-2)\n",
    "    data = data.withColumn(\"week_of_year\", weekofyear(col(\"pickup_datetime\")))\n",
    "    data = data.withColumn(\"month_of_year\", month(col(\"pickup_datetime\")))\n",
    "    data = data.withColumn(\"quarter_of_year\", quarter(col(\"pickup_datetime\")))\n",
    "    data = data.withColumn(\"year\", year(col(\"pickup_datetime\")))\n",
    "    \n",
    "    @udf(\"int\")\n",
    "    def night(hour, weekday):\n",
    "        if ((hour <= 20) and (hour >= 16) and (weekday < 5)):\n",
    "            return int(1)\n",
    "        else:\n",
    "            return int(0)\n",
    "\n",
    "    @udf(\"int\")\n",
    "    def late_night(hour):\n",
    "        if ((hour <= 6) and (hour >= 20)):\n",
    "            return int(1)\n",
    "        else:\n",
    "            return int(0)\n",
    "    data = data.withColumn(\"night\", night(\"hour_of_day\", \"day_of_week\"))\n",
    "    data = data.withColumn(\"late_night\", late_night(\"hour_of_day\"))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add distance related features\n",
    "def add_distance_features(data):\n",
    "\n",
    "    @udf(\"float\")\n",
    "    def manhattan(lat1, lon1, lat2, lon2):\n",
    "        return float(np.abs(lat2 - lat1) + np.abs(lon2 - lon1))\n",
    "    \n",
    "    # Location of NYC downtown\n",
    "    ny = (-74.0063889, 40.7141667)\n",
    "    # Location of the three airport in NYC\n",
    "    jfk = (-73.7822222222, 40.6441666667)\n",
    "    ewr = (-74.175, 40.69)\n",
    "    lgr = (-73.87, 40.77)\n",
    "    \n",
    "    # Feature about the distance from pickup and dropoff location to airports\n",
    "    data = data.withColumn(\"abs_diff_longitude\", abs(col(\"dropoff_longitude\")-col(\"pickup_longitude\"))) \\\n",
    "            .withColumn(\"abs_diff_latitude\", abs(col(\"dropoff_latitude\") - col(\"pickup_latitude\")))\n",
    "    data = data.withColumn(\"manhattan\", col(\"abs_diff_latitude\")+col(\"abs_diff_longitude\"))\n",
    "    data = data.withColumn(\"pickup_distance_jfk\", manhattan(\"pickup_longitude\", \"pickup_latitude\", lit(jfk[0]), lit(jfk[1])))\n",
    "    data = data.withColumn(\"dropoff_distance_jfk\", manhattan(\"dropoff_longitude\", \"dropoff_latitude\", lit(jfk[0]), lit(jfk[1])))\n",
    "    data = data.withColumn(\"pickup_distance_ewr\", manhattan(\"pickup_longitude\", \"pickup_latitude\", lit(ewr[0]), lit(ewr[1])))\n",
    "    data = data.withColumn(\"dropoff_distance_ewr\", manhattan(\"dropoff_longitude\", \"dropoff_latitude\", lit(ewr[0]), lit(ewr[1])))\n",
    "    data = data.withColumn(\"pickup_distance_lgr\", manhattan(\"pickup_longitude\", \"pickup_latitude\", lit(lgr[0]), lit(lgr[1])))\n",
    "    data = data.withColumn(\"dropoff_distance_lgr\", manhattan(\"dropoff_longitude\", \"dropoff_latitude\", lit(lgr[0]), lit(lgr[1])))\n",
    "    data = data.withColumn(\"pickup_distance_downtown\", manhattan(\"pickup_longitude\", \"pickup_latitude\", lit(ny[0]), lit(ny[1])))\n",
    "    data = data.withColumn(\"dropoff_distance_downtown\", manhattan(\"dropoff_longitude\", \"dropoff_latitude\", lit(ny[0]), lit(ny[1])))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unused features\n",
    "def drop_col(data):\n",
    "    \n",
    "    data = data.drop(\"pickup_datetime\") \\\n",
    "            .drop(\"pickup_longitude\") \\\n",
    "            .drop(\"pickup_latitude\") \\\n",
    "            .drop(\"dropoff_longitude\") \\\n",
    "            .drop(\"dropoff_latitude\") \\\n",
    "            .drop(\"passenger_count\") \\\n",
    "            .drop(\"key\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = clean_up(train)\n",
    "\n",
    "train_data = add_time_features(train_data)\n",
    "\n",
    "train_data = add_distance_features(train_data)\n",
    "\n",
    "train_data = drop_col(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed model training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset into training and evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = random_split(train_data, [0.9, 0.1])\n",
    "features = [field.name for field in list(train_df.schema) if field.name != \"fare_amount\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the keras model  \n",
    "Each feature will be regarded as an input with shape (1,ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "inTensor = []\n",
    "for _ in range(len(features)):\n",
    "    inTensor.append(keras.Input((1,)))\n",
    "    \n",
    "concatenated = keras.layers.concatenate(inTensor)\n",
    "fc1 = keras.layers.Dense(256, activation='relu')(concatenated)\n",
    "bn1 = keras.layers.BatchNormalization()(fc1)\n",
    "fc2 = keras.layers.Dense(128, activation='relu')(bn1)\n",
    "bn2 = keras.layers.BatchNormalization()(fc2)\n",
    "fc3 = keras.layers.Dense(64, activation='relu')(bn2)\n",
    "bn3 = keras.layers.BatchNormalization()(fc3)\n",
    "fc4 = keras.layers.Dense(32, activation='relu')(bn3)\n",
    "bn4 = keras.layers.BatchNormalization()(fc4)\n",
    "fc5 = keras.layers.Dense(16, activation='relu')(bn4)\n",
    "bn5 = keras.layers.BatchNormalization()(fc5)\n",
    "fc6 = keras.layers.Dense(1)(bn5)\n",
    "\n",
    "model = keras.models.Model(inTensor, fc6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the optimizer and loss function  \n",
    "Then create the tensorflow estimator provided by Raydp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = keras.optimizers.Adam(lr=0.001)\n",
    "loss = keras.losses.MeanSquaredError()\n",
    "estimator = TFEstimator(num_workers=6, model=model, optimizer=adam, loss=loss, metrics=[\"mae\"],\n",
    "                       feature_columns=features, label_column=\"fare_amount\", batch_size=256, num_epochs=30,\n",
    "                        config={\"fit_config\": {\"steps_per_epoch\": train_df.count() // 256}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit_on_spark(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.shutdown()\n",
    "raydp.stop_spark()\n",
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
