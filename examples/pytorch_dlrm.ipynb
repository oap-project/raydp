{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "# Copyright (c) 2020 NVIDIA CORPORATION. All rights reserved.\n",
    "# Modifications copyright Intel\n",
    "# \n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#       http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An implementation of a deep learning recommendation model (DLRM)\n",
    "This is a modified version of DRLM implementation . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocess with Apache Spark\n",
    "The following functions are used for data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "del os.environ[\"http_proxy\"]\n",
    "del os.environ[\"https_proxy\"]\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from operator import itemgetter\n",
    "from time import time\n",
    "\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COL = 0\n",
    "INT_COLS = list(range(1, 14))\n",
    "CAT_COLS = list(range(14, 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_benchmark = {}\n",
    "\n",
    "@contextmanager\n",
    "def _timed(step):\n",
    "    start = time()\n",
    "    yield\n",
    "    end = time()\n",
    "    _benchmark[step] = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_counts_with_frequency_limit(df, frequency_limit=None):\n",
    "    # column_id, data, count\n",
    "    cols = ['_c%d' % i for i in CAT_COLS]\n",
    "    df = (df\n",
    "          .select(posexplode(array(*cols)))\n",
    "          .withColumnRenamed('pos', 'column_id')\n",
    "          .withColumnRenamed('col', 'data')\n",
    "          .filter('data is not null')\n",
    "          .groupBy('column_id', 'data')\n",
    "          .count())\n",
    "\n",
    "    if frequency_limit:\n",
    "        frequency_limit = frequency_limit.split(\",\")\n",
    "        exclude = []\n",
    "        default_limit = None\n",
    "        for fl in frequency_limit:\n",
    "            frequency_pair = fl.split(\":\")\n",
    "            if len(frequency_pair) == 1:\n",
    "                default_limit = int(frequency_pair[0])\n",
    "            elif len(frequency_pair) == 2:\n",
    "                df = df.filter((col('column_id') != int(frequency_pair[0]) - CAT_COLS[0]) | (\n",
    "                        col('count') >= int(frequency_pair[1])))\n",
    "                exclude.append(int(frequency_pair[0]))\n",
    "        if default_limit:\n",
    "            remain = [x - CAT_COLS[0] for x in CAT_COLS if x not in exclude]\n",
    "            df = df.filter((~col('column_id').isin(remain)) | (col('count') >= default_limit))\n",
    "            # for comparing isin and separate filter\n",
    "            # for i in remain:\n",
    "            #     df = df.filter((col('column_id') != i - CAT_COLS[0]) | (col('count') >= default_limit))\n",
    "    return df\n",
    "\n",
    "\n",
    "def assign_id_with_window(df):\n",
    "    # column_id, data, model_count, id\n",
    "    windowed = Window.partitionBy('column_id').orderBy(desc('count'))\n",
    "    return (df\n",
    "            .withColumn('id', row_number().over(windowed) - 1)\n",
    "            .withColumnRenamed('count', 'model_count'))\n",
    "\n",
    "\n",
    "def get_column_models(combined_model):\n",
    "    for i in CAT_COLS:\n",
    "        # data, model_count, id\n",
    "        model = (combined_model\n",
    "                 .filter('column_id == %d' % (i - CAT_COLS[0]))\n",
    "                 .drop('column_id'))\n",
    "        yield i, model\n",
    "\n",
    "\n",
    "def col_of_rand_long():\n",
    "    return (rand() * (1 << 52)).cast(LongType())\n",
    "\n",
    "\n",
    "def apply_models(df, models):\n",
    "    for i, model in models:\n",
    "        col_name = '_c%d' % i\n",
    "        model = model.drop('model_count').withColumnRenamed('data', col_name)\n",
    "        df = (df\n",
    "              .join(model, col_name, how='left')\n",
    "              .drop(col_name)\n",
    "              .withColumnRenamed('id', col_name))\n",
    "    return df.fillna(0, ['_c%d' % i for i in CAT_COLS])\n",
    "\n",
    "\n",
    "def transform_log(df, transform_log=False):\n",
    "    cols = ['_c%d' % i for i in INT_COLS]\n",
    "    if transform_log:\n",
    "        for col_name in cols:\n",
    "            df = df.withColumn(col_name, log(df[col_name] + 3))\n",
    "    return df.fillna(0, cols)\n",
    "\n",
    "\n",
    "def rand_ordinal(df):\n",
    "    # create a random long from the double precision float.\n",
    "    # The fraction part of a double is 52 bits, so we try to capture as much\n",
    "    # of that as possible\n",
    "    return df.withColumn('ordinal', col_of_rand_long())\n",
    "\n",
    "\n",
    "def process_column_models(column_models):\n",
    "    for i, column in column_models:\n",
    "        values = column.agg(count('*').alias('size')).collect()\n",
    "        yield i, column, values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(df,\n",
    "                frequency_limit: str = 3,\n",
    "                output_ordering: str = \"total_random\",\n",
    "                no_numeric_log_col: bool = False):\n",
    "    with _timed(\"generate models\"):\n",
    "        # column_id, data, count\n",
    "        col_counts = get_column_counts_with_frequency_limit(df, frequency_limit=frequency_limit)\n",
    "        # column_id, data, model_count, id\n",
    "        col_counts_with_id = assign_id_with_window(col_counts)\n",
    "        # data, model_count, id\n",
    "        columns_model = list(get_column_models(col_counts_with_id))\n",
    "\n",
    "    with _timed('transform'):\n",
    "        # 40 columns, ordinal, day\n",
    "        if output_ordering == 'total_random':\n",
    "            df = rand_ordinal(df)\n",
    "        elif output_ordering == 'input':\n",
    "            df = df.withColumn('ordinal', monotonically_increasing_id())\n",
    "        else:  # any ordering\n",
    "            pass\n",
    "\n",
    "        models = list(process_column_models(columns_model))\n",
    "        categorical_feature_sizes = [agg.size for _, _, agg in models]\n",
    "        models = [(i, df) for i, df, agg in models]\n",
    "\n",
    "        df = apply_models(df, models)\n",
    "        df = transform_log(df, not no_numeric_log_col)\n",
    "\n",
    "        if output_ordering == 'total_random':\n",
    "            # Don't do a full sort it is expensive. Order is random so\n",
    "            # just make it random\n",
    "            df = df.repartition('ordinal').sortWithinPartitions('ordinal')\n",
    "            df = df.drop('ordinal')\n",
    "        elif output_ordering == 'input':\n",
    "            # Applying the dictionary happened within a single task so we are already really\n",
    "            # close to the correct order, just need to sort within the partition\n",
    "            df = df.sortWithinPartitions('ordinal')\n",
    "            df = df.drop('ordinal')\n",
    "            df = df.drop(\"day\")\n",
    "        # else: any ordering so do nothing the ordering does not matter\n",
    "\n",
    "    return df, categorical_feature_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DLRM Model\n",
    "Define the DLRM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "from typing import List, Sequence, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class DotInteraction:\n",
    "\n",
    "    def __init__(self, embedding_num: int, embedding_dim: int):\n",
    "        \"\"\"\n",
    "        Interactions are among outputs of all the embedding tables and bottom MLP, total number of\n",
    "        (num_embedding_tables + 1) vectors with size embedding_dim. ``dot`` product interaction computes dot product\n",
    "        between any 2 vectors. Output of interaction will have shape [num_interactions, embedding_dim].\n",
    "        \"\"\"\n",
    "        self._num_interaction_inputs = embedding_num + 1\n",
    "        self._embedding_dim = embedding_dim\n",
    "        self._tril_indices = torch.tensor([[i for i in range(self._num_interaction_inputs)\n",
    "                                            for _ in range(i)],\n",
    "                                           [j for i in range(self._num_interaction_inputs)\n",
    "                                            for j in range(i)]])\n",
    "\n",
    "    @property\n",
    "    def num_interactions(self) -> int:\n",
    "        n = (self._num_interaction_inputs * (self._num_interaction_inputs - 1)) // 2 + self._embedding_dim\n",
    "        return n + 1  # pad 1 to be multiple of 8\n",
    "\n",
    "    def interact(self, bottom_output, bottom_mlp_output):\n",
    "        \"\"\"\n",
    "        :param bottom_output: [batch_size, 1 + #embeddings, embedding_dim]\n",
    "        :param bottom_mlp_output\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        batch_size = bottom_output.size()[0]\n",
    "\n",
    "        interaction = torch.bmm(bottom_output, torch.transpose(bottom_output, 1, 2))\n",
    "        interaction_flat = interaction[:, self._tril_indices[0], self._tril_indices[1]]\n",
    "\n",
    "        # concatenate dense features and interactions\n",
    "        zeros_padding = torch.zeros(batch_size, 1, dtype=bottom_output.dtype, device=bottom_output.device)\n",
    "        interaction_output = torch.cat(\n",
    "            (bottom_mlp_output, interaction_flat, zeros_padding), dim=1)\n",
    "\n",
    "        return interaction_output\n",
    "\n",
    "\n",
    "class TorchMlp(nn.Module):\n",
    "    def __init__(self, input_dim: int, sizes: Sequence[int]):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        for output_dims in sizes:\n",
    "            layers.append(nn.Linear(input_dim, output_dims))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            input_dim = output_dims\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.normal_(module.weight.data, 0., math.sqrt(2. / (module.in_features + module.out_features)))\n",
    "                nn.init.normal_(module.bias.data, 0., math.sqrt(1. / module.out_features))\n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "        return [layer.weight for layer in self.layers if isinstance(layer, nn.Linear)]\n",
    "\n",
    "    @property\n",
    "    def biases(self):\n",
    "        return [layer.bias for layer in self.layers if isinstance(layer, nn.Linear)]\n",
    "\n",
    "    def forward(self, mlp_input: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mlp_input (Tensor): with shape [batch_size, num_features]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Mlp output in shape [batch_size, num_output_features]\n",
    "        \"\"\"\n",
    "        return self.layers(mlp_input)\n",
    "\n",
    "\n",
    "class MultiTableEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            categorical_feature_sizes: Sequence[int],\n",
    "            embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self._categorical_feature_sizes = copy.copy(categorical_feature_sizes)\n",
    "\n",
    "        embeddings = []\n",
    "        # Each embedding table has size [num_features, embedding_dim]\n",
    "        for i, num_features in enumerate(categorical_feature_sizes):\n",
    "            embedding_weight = torch.empty((num_features, embedding_dim))\n",
    "            embedding = nn.Embedding.from_pretrained(embedding_weight, freeze=False, sparse=True)\n",
    "            embeddings.append(embedding)\n",
    "\n",
    "        self.embeddings = nn.ModuleList(embeddings)\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    def forward(self, categorical_inputs) -> List[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            categorical_inputs (Tensor): with shape [batch_size, num_categorical_features]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: embedding outputs in shape [batch, embedding_num, embedding_dim]\n",
    "        \"\"\"\n",
    "        # embedding_outputs will be a list of (26 in the case of Criteo) fetched embeddings with shape\n",
    "        # [batch_size, embedding_size]\n",
    "        embedding_outputs = []\n",
    "        for embedding_id, embedding in enumerate(self.embeddings):\n",
    "            embedding_outputs.append(embedding(categorical_inputs[:, embedding_id]).unsqueeze(1))\n",
    "\n",
    "        return embedding_outputs\n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "        return [embedding.weight.data for embedding in self.embeddings]\n",
    "\n",
    "\n",
    "class DlrmBottom(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_numerical_features: int,\n",
    "            categorical_feature_sizes: Sequence[int],\n",
    "            bottom_mlp_sizes: Optional[Sequence[int]] = None,\n",
    "            embedding_dim: int = 128):\n",
    "        super().__init__()\n",
    "        assert bottom_mlp_sizes is None or embedding_dim == bottom_mlp_sizes[-1], \"The last bottom MLP layer must\" \\\n",
    "                                                                                  \" have same size as embedding.\"\n",
    "        self._embedding_dim = embedding_dim\n",
    "        self._categorical_feature_sizes = copy.copy(categorical_feature_sizes)\n",
    "\n",
    "        self.embeddings = MultiTableEmbeddings(categorical_feature_sizes, embedding_dim)\n",
    "        self.mlp = (TorchMlp(num_numerical_features, bottom_mlp_sizes)\n",
    "                    if bottom_mlp_sizes else torch.nn.ModuleList())\n",
    "\n",
    "        self._initialize_embeddings_weights(self.embeddings, categorical_feature_sizes)\n",
    "\n",
    "    def _initialize_embeddings_weights(self, embeddings, categorical_feature_sizes: Sequence[int]):\n",
    "        assert len(embeddings.weights) == len(categorical_feature_sizes)\n",
    "\n",
    "        for size, weight in zip(categorical_feature_sizes, embeddings.weights):\n",
    "            nn.init.uniform_(\n",
    "                weight,\n",
    "                -math.sqrt(1. / size),\n",
    "                math.sqrt(1. / size)\n",
    "            )\n",
    "\n",
    "    @property\n",
    "    def num_categorical_features(self) -> int:\n",
    "        return len(self._categorical_feature_sizes)\n",
    "\n",
    "    @property\n",
    "    def num_feature_vectors(self) -> int:\n",
    "        return self.num_categorical_features + int(self.mlp is not None)\n",
    "\n",
    "    def forward(self, numerical_input, categorical_inputs) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            numerical_input (Tensor): with shape [batch_size, num_numerical_features]\n",
    "            categorical_inputs (Tensor): with shape [batch_size, num_categorical_features]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Concatenated bottom mlp and embedding output in shape [batch, 1 + #embedding, embedding_dim]\n",
    "        \"\"\"\n",
    "        batch_size = len(numerical_input) if numerical_input is not None else len(categorical_inputs)\n",
    "        bottom_output = []\n",
    "        bottom_mlp_output = None\n",
    "\n",
    "        if self.mlp:\n",
    "            bottom_mlp_output = self.mlp(numerical_input)\n",
    "            # reshape bottom mlp to concatenate with embeddings\n",
    "            bottom_output.append(bottom_mlp_output.view(batch_size, 1, -1))\n",
    "\n",
    "        if self.num_categorical_features > 0:\n",
    "            bottom_output += self.embeddings(categorical_inputs)\n",
    "\n",
    "        if len(bottom_output) == 1:\n",
    "            return bottom_output[0], bottom_mlp_output\n",
    "\n",
    "        return torch.cat(bottom_output, dim=1), bottom_mlp_output\n",
    "\n",
    "\n",
    "class DlrmTop(nn.Module):\n",
    "\n",
    "    def __init__(self, top_mlp_sizes: Sequence[int], interaction: DotInteraction):\n",
    "        super().__init__()\n",
    "\n",
    "        self.interaction = interaction\n",
    "        self.mlp = TorchMlp(interaction.num_interactions, top_mlp_sizes[:-1])\n",
    "        self.out = nn.Linear(top_mlp_sizes[-2], top_mlp_sizes[-1])\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        # Explicitly set weight corresponding to zero padded interaction output. They will\n",
    "        # stay 0 throughout the entire training. An assert can be added to the end of the training\n",
    "        # to prove it doesn't increase model capacity but just 0 paddings.\n",
    "        nn.init.zeros_(self.mlp.weights[0][:, -1].data)\n",
    "\n",
    "    def forward(self, bottom_output, bottom_mlp_output):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            bottom_output (Tensor): with shape [batch_size, 1 + #embeddings, embedding_dim]\n",
    "            bottom_mlp_output (Tensor): with shape [batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        interaction_output = self.interaction.interact(bottom_output, bottom_mlp_output)\n",
    "        return self.out(self.mlp(interaction_output))\n",
    "\n",
    "\n",
    "class Dlrm(nn.Module):\n",
    "    \"\"\"Reimplement Facebook's DLRM model\n",
    "\n",
    "    Original implementation is from https://github.com/facebookresearch/dlrm.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_numerical_features: int,\n",
    "            categorical_feature_sizes: Sequence[int],\n",
    "            bottom_mlp_sizes: Sequence[int],\n",
    "            top_mlp_sizes: Sequence[int],\n",
    "            embedding_dim: int = 32):\n",
    "        super().__init__()\n",
    "        assert embedding_dim == bottom_mlp_sizes[-1], \"The last bottom MLP layer must have same size as embedding.\"\n",
    "\n",
    "        self.num_numerical_features = num_numerical_features\n",
    "\n",
    "        interaction = DotInteraction(len(categorical_feature_sizes), embedding_dim)\n",
    "\n",
    "        self.bottom_model = DlrmBottom(\n",
    "            num_numerical_features=num_numerical_features,\n",
    "            categorical_feature_sizes=categorical_feature_sizes,\n",
    "            bottom_mlp_sizes=bottom_mlp_sizes,\n",
    "            embedding_dim=embedding_dim)\n",
    "        self.top_model = DlrmTop(top_mlp_sizes, interaction)\n",
    "\n",
    "    def forward(self, *inputs):\n",
    "        inputs = list(inputs)\n",
    "        numerical_input = torch.cat(inputs[0: self.num_numerical_features], dim=1)\n",
    "        categorical_inputs = torch.cat(inputs[self.num_numerical_features:], dim=1)\n",
    "        bottom_output, bottom_mlp_output = self.bottom_model(numerical_input, categorical_inputs)\n",
    "        return self.top_model(bottom_output, bottom_mlp_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the data preprocess and model training with RayDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import raydp\n",
    "from raydp.utils import random_split\n",
    "from raydp.torch import TorchEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to ray cluster or init\n",
    "ray.init(_redis_password=\"123\")\n",
    "# start up spark\n",
    "spark = raydp.init_spark(app_name=\"dlrm\",\n",
    "                         num_executors=2,\n",
    "                         executor_cores=4,\n",
    "                         executor_memory=\"8GB\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data can download from: https://www.kaggle.com/c/criteo-display-ad-challenge/\n",
    "file_path = \"/Users/xianyang/datasets/criteo-small/train_1m.txt\"\n",
    "\n",
    "# define the df schmea\n",
    "label_fields = [StructField('_c%d' % LABEL_COL, IntegerType())]\n",
    "int_fields = [StructField('_c%d' % i, IntegerType()) for i in INT_COLS]\n",
    "str_fields = [StructField('_c%d' % i, StringType()) for i in CAT_COLS]\n",
    "\n",
    "schema = StructType(label_fields + int_fields + str_fields)\n",
    "df = spark.read.schema(schema).option(\"sep\", \"\\t\").csv(file_path)\n",
    "df.cache()\n",
    "df, categorical_feature_sizes = pre_process(df=df,\n",
    "                                            frequency_limit=\"3\",\n",
    "                                            output_ordering=\"total_random\",\n",
    "                                            no_numeric_log_col=False)\n",
    "train_df, test_df = random_split(df, [0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Dlrm(num_numerical_features=len(INT_COLS),\n",
    "             categorical_feature_sizes=categorical_feature_sizes,\n",
    "             bottom_mlp_sizes=[512, 128, 32],\n",
    "             top_mlp_sizes=[1024, 1024, 512, 256, 1])\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "feature_columns = ['_c%d' % i for i in range(1, 40)]\n",
    "feature_types = [torch.float32] * len(INT_COLS)\n",
    "feature_types += [torch.long] * len(CAT_COLS)\n",
    "\n",
    "estimator = TorchEstimator(num_workers=2,\n",
    "                           model=model,\n",
    "                           optimizer=optimizer,\n",
    "                           loss=loss_fn,\n",
    "                           feature_columns=feature_columns,\n",
    "                           feature_types=feature_types,\n",
    "                           label_column=\"_c0\",\n",
    "                           label_type=torch.float,\n",
    "                           batch_size=128,\n",
    "                           num_epochs=2,\n",
    "                           use_gpu=False)\n",
    "\n",
    "estimator.fit_on_spark(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raydp.stop_spark()\n",
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('raydp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "a245fd77c683bafec11b88c8942e774aac894382a1d1374b683647c914121f01"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
