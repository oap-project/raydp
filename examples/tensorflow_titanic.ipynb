{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Survival Prediction with RayDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import os\n",
    "import re\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "import raydp\n",
    "from raydp.tf import TFEstimator\n",
    "from raydp.utils import random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize or connect to existed Ray cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, you need to init or connect to a ray cluster. Note that you should set include_java to True(For ray 0.8.7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After initializing ray cluster, you can use the raydp api to get a spark session  \n",
    "`init_spark` take 4 required parameters and 1 optional parameters:  \n",
    "1. app_name: the application name\n",
    "2. num_executors: number of executors for spark application\n",
    "3. cores_per_executor: number of cores for each executor\n",
    "4. executor_memory: memory size for each executor \n",
    "5. config[option]: extra config for spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_name = \"Titanic survival prediction with RayDp\"\n",
    "num_executors = 4\n",
    "cores_per_executor = 1\n",
    "memory_per_executor = \"1GB\"\n",
    "spark = raydp.init_spark(app_name, num_executors, cores_per_executor, memory_per_executor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed data preprocessing with pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can use pyspark api for distributed data preprocessing with the spark session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to download the dataset from https://www.kaggle.com/c/titanic/data  \n",
    "Read it with spark session and you will get a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = spark.read.format(\"csv\").option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(\"/mnt/DP_disk8/dataset/titanic_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then do data processing and feature engineering with spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_na(data):\n",
    "    \n",
    "    # Fill NA in column Fare, Age and Embarked\n",
    "    data = data.fillna({\"Embarked\": \"S\"})\n",
    "    \n",
    "    fare_avg = data.select(mean(col(\"Fare\")).alias(\"mean\")).collect()\n",
    "    data = data.na.fill({\"Fare\": fare_avg[0][\"mean\"]})\n",
    "    \n",
    "    age_avg = data.select(mean(col(\"Age\")).alias(\"mean\")).collect()\n",
    "    data = data.na.fill({'Age': age_avg[0][\"mean\"]})\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_features(data):\n",
    "    \n",
    "    # Add some new features\n",
    "    data = data.withColumn(\"name_length\", length(\"Name\"))\n",
    "    data = data.withColumn(\"has_cabin\", col(\"Cabin\").isNotNull().cast('int'))\n",
    "    \n",
    "    data = data.withColumn(\"family_size\", col(\"SibSp\") + col(\"Parch\") + 1)\n",
    "    data = data.withColumn(\"is_alone\", (col(\"family_size\") == 1).cast('int'))\n",
    "    \n",
    "    \n",
    "    # Add some features about passengers' title with spark udf\n",
    "    @udf(\"string\")\n",
    "    def get_title(name):\n",
    "        title = ''\n",
    "        title_match = re.search(' ([A-Za-z]+)\\.', name)\n",
    "        if (title_match):\n",
    "            title = title_match.group(1)\n",
    "            if (title in ['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\n",
    "                          'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona']):\n",
    "                title = 'Rare'\n",
    "            return title\n",
    "        return title\n",
    "    data = data.withColumn(\"Title\", get_title(col(\"Name\")))\n",
    "    data = data.withColumn(\"Title\", regexp_replace(\"Title\", \"Mlle|Ms\", \"Miss\"))\n",
    "    data = data.withColumn(\"Title\", regexp_replace(\"Title\", \"Mme\", \"Mrs\"))\n",
    "    \n",
    "    # Encode column Sex\n",
    "    sex_udf = udf(lambda x: 0 if x == \"female\" else 1)\n",
    "    data = data.withColumn(\"Sex\", sex_udf(col(\"Sex\")).cast('int'))\n",
    "    \n",
    "    # Encode column Title\n",
    "    title_map = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
    "    title_udf = udf(lambda x: title_map[x])\n",
    "    data = data.withColumn(\"Title\", title_udf(col(\"Title\")).cast('int'))\n",
    "    \n",
    "    # Encode column Embarked\n",
    "    embarked_map = {'S': 0, 'C': 1, 'Q': 2}\n",
    "    embarked_udf = udf(lambda x: embarked_map[x])\n",
    "    data = data.withColumn(\"Embarked\", embarked_udf(col(\"Embarked\")).cast('int'))\n",
    "    \n",
    "    # Categorize column Fare\n",
    "    @udf(\"int\")\n",
    "    def fare_map(fare):\n",
    "        if (fare <= 7.91):\n",
    "            return 0\n",
    "        elif fare <= 14.454:\n",
    "            return 1\n",
    "        elif fare <= 31:\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "    data = data.withColumn(\"Fare\", fare_map(col(\"Fare\")))\n",
    "    \n",
    "    # Categorize column Age\n",
    "    @udf(\"int\")\n",
    "    def age_map(age):\n",
    "        if age <= 16:\n",
    "            return 0\n",
    "        elif age <= 32:\n",
    "            return 1\n",
    "        elif age <= 48:\n",
    "            return 2\n",
    "        elif age <= 64:\n",
    "            return 3\n",
    "        else:\n",
    "            return 4\n",
    "    data = data.withColumn(\"Age\", age_map(col(\"Age\")))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_cols(data):\n",
    "    \n",
    "    # Drop useless columns\n",
    "    data = data.drop(\"PassengerId\") \\\n",
    "        .drop(\"Name\") \\\n",
    "        .drop(\"Ticket\") \\\n",
    "        .drop(\"Cabin\") \\\n",
    "        .drop(\"SibSp\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = fill_na(train)\n",
    "\n",
    "train = do_features(train)\n",
    "\n",
    "train = drop_cols(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df, test_df = random_split(train, [0.95, 0.05])\n",
    "features = [field.name for field in list(train.schema) if field.name != \"Survived\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the keras model  \n",
    "Each feature will be regarded as an input with shape (1,) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inTensor = []\n",
    "for _ in range(len(features)):\n",
    "    inTensor.append(keras.Input((1,)))\n",
    "concatenated = keras.layers.concatenate(inTensor)\n",
    "fc1 = keras.layers.Dense(32, activation='relu')(concatenated)\n",
    "fc2 = keras.layers.Dense(32, activation='relu')(fc1)\n",
    "dp1 = keras.layers.Dropout(0.25)(fc2)\n",
    "fc3 = keras.layers.Dense(16, activation='relu')(dp1)\n",
    "dp2 = keras.layers.Dropout(0.25)(fc3)\n",
    "fc4 = keras.layers.Dense(1, activation='sigmoid')(dp2)\n",
    "model = keras.models.Model(inTensor, fc4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the optimizer and loss function  \n",
    "Then create the tensorflow estimator provided by Raydp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsp = keras.optimizers.RMSprop()\n",
    "loss = keras.losses.BinaryCrossentropy()\n",
    "estimator = TFEstimator(num_workers=4, model=model, optimizer=rmsp, loss=loss, metrics=[\"binary_accuracy\"],\n",
    "                        feature_columns=features, label_columns=\"Survived\", batch_size=32, num_epochs=100,\n",
    "                        config={\"fit_config\": {\"steps_per_epoch\": train.count() // 32}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit_on_spark(train, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown raydp and spark\n",
    "raydp.stop_spark()\n",
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('raydp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "a245fd77c683bafec11b88c8942e774aac894382a1d1374b683647c914121f01"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
