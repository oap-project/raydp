{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC_Taxi Fare Prediction with RayDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from raydp.spark import context\n",
    "from raydp.spark.torch.estimator import TorchEstimator\n",
    "from raydp.spark.utils import random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize or connect to existed Ray cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly, You need to init or connect to a ray cluster. Note that you should set include_java to True.\n",
    "# For more config info in ray, please refer the ray doc. https://docs.ray.io/en/latest/package-ref.html\n",
    "# ray.init(address=\"auto\", redis_password=\"123\")\n",
    "ray.init(include_java=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spark session based on RayDP API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After initialize ray cluster, you can use the raydp api to get a spark session\n",
    "app_name = \"NYC_Taxi Fare Prediction with RayDP\"\n",
    "num_executors = 2\n",
    "cores_per_executor = 1\n",
    "memory_per_executor = \"5GB\"\n",
    "spark = context.init_spark(app_name, num_executors, cores_per_executor, memory_per_executor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed data preprocessing with pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then you can operate as you are using spark\n",
    "# The dataset can be downloaded from https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/data\n",
    "data = spark.read.format(\"csv\").option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(\"/mnt/DP_disk8/nyc_taxi.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter data\n",
    "data = data.select(\"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\", \"passenger_count\", \"fare_amount\") \\\n",
    "    .filter(data['pickup_longitude'] <= -73.0)  \\\n",
    "    .filter(data['pickup_longitude'] >= -74.3)  \\\n",
    "    .filter(data['dropoff_longitude'] <= -73.0) \\\n",
    "    .filter(data['dropoff_longitude'] >= -74.3) \\\n",
    "    .filter(data['pickup_latitude'] <= 41.7)    \\\n",
    "    .filter(data['pickup_latitude'] >= 40.6)    \\\n",
    "    .filter(data['dropoff_latitude'] <= 41.7)   \\\n",
    "    .filter(data['dropoff_latitude'] >= 40.6)   \\\n",
    "    .filter(data['passenger_count'] <= 10)      \\\n",
    "    .filter(data['fare_amount'] >= 0.0)         \\\n",
    "    .filter(abs(data['dropoff_longitude'] - data['pickup_longitude']) <= 5) \\\n",
    "    .filter(abs(data['dropoff_latitude'] - data['pickup_latitude']) <= 5)\n",
    "# add two new features\n",
    "data = data.withColumn(\"abs_diff_longitude\", abs(col(\"dropoff_longitude\") - col(\"pickup_longitude\"))) \\\n",
    "           .withColumn(\"abs_diff_latitude\", abs(col(\"dropoff_latitude\") - col(\"pickup_latitude\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train_dataset and test_dataset\n",
    "train_df, test_df = random_split(data, [0.7, 0.3])\n",
    "# train_df = train_data_select.sample(False, 0.0005)\n",
    "# test_df = train_data_select.sample(False, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model, loss function and optimizer\n",
    "class NYC_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NYC_Model, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(7, 14)\n",
    "        self.fc2 = nn.Linear(14, 7)\n",
    "        self.fc3 = nn.Linear(7, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "nyc_model = NYC_Model()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(nyc_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the estimator which is a scikit-learn like API for Torch distributed model training\n",
    "features = [field.name for field in list(train_data_select.schema) if field.name != \"fare_amount\"]\n",
    "estimator = TorchEstimator(num_workers=4,\n",
    "                           model=nyc_model,\n",
    "                           optimizer=optimizer,\n",
    "                           loss=criterion, \n",
    "                           feature_columns=features,\n",
    "                           label_column=\"fare_amount\",\n",
    "                           batch_size=100,\n",
    "                           num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distributed training the model\n",
    "estimator.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "estimator.evaluate(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutdown raydp and ray\n",
    "estimator.shutdown()\n",
    "context.stop_spark()\n",
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
